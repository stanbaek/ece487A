{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82970c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://colab.research.google.com/github/stanbaek/ece487/blob/main/docs/Labs/Lab3_CameraCalibration.ipynb\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    "\n",
    "# ‚ùÑÔ∏è Lab3: Camera Calibration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80063cda-a0bd-4f1a-a27e-6c7c0421d284",
   "metadata": {},
   "source": [
    "**A note on this document**\n",
    "This document is known as a Jupyter notebook; it is used in academia and industry to allow text and executable code to coexist in a very easy to read format. Blocks can contain text or executable code. To run the executable code in this notebook, click <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://colab.research.google.com/github/stanbaek/ece487/blob/main/docs/Labs/Lab3_CameraCalibration.ipynb\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    ". For blocks containing code, press `Shift + Enter`, `Ctrl+Enter`, or click the arrow on the block to run the code. Earlier blocks of code need to be run for the later blocks of code to work.\n",
    "\n",
    "```{caution}\n",
    "Interactive matplotlib figures do not seem to work properly in jupyter notebook and Google Colab. ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef1577-7967-46c0-9c8a-231b8606f7f1",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The purpose of this lab is to calibrate the OpenMV camera that will be used in your Project 2.\n",
    "\n",
    "```{image} ./figures/AprilTagBlocks.jpg\n",
    ":width: 500\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad395e-1f30-4c5b-8f80-35435608d2dd",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "### AprilTags\n",
    "\n",
    "AprilTag is a visual fiducial system, useful for various tasks, including augmented reality, robotics, and camera calibration. We will learn how fiducial markers are used in image processing. Specifically, we will utilize MicroPython to identify different AprilTags and use the 3D position and orientation to determine the distance of an object from the camera.\n",
    "\n",
    "```{image} ./figures/AprilTags.png\n",
    ":width: 500\n",
    ":align: center\n",
    "```\n",
    "\n",
    "A fiducial marker is an artificial feature used in creating controllable experiments, ground truthing, and simplifying systems development where perception is not the central objective. Examples of fiducial markers include ArUco Markers, AprilTags, and QR codes. Each of these different tags holds information such as an ID or, in the case of QR codes, websites, messages, etc. We will primarily focus on AprilTags as a very robust Python package is already built. This library identifies AprilTags and will provide information about the tag‚Äôs size, distance, and orientation.\n",
    "\n",
    "ref: https://april.eecs.umich.edu/software/apriltag\n",
    "\n",
    "\n",
    "## üíª Procedure\n",
    "\n",
    "1. **Read this page thoroughly.**\n",
    "2. Then, go to Colab by clicking the icon at the end of this page and complete the sensor calibration. **AR means action required in Colab.**\n",
    "\n",
    "### OpenMV Cam\n",
    "\n",
    "Go to https://openmv.io/pages/download to download the latest OpenMV IDE.  Install the software on your computer.\n",
    "\n",
    "- Connect the camera to your computer and run the software. \n",
    "- Download `find_apriltags_3d_pose_4.py` from Teams.\n",
    "- In OpenMV IDE, go to File > Open File and select `find_apriltags_3d_pose_4.py` to load.\n",
    "- Click the Connect button on the bottom left of the IDE.  \n",
    "- Click the Start button (green arrow).\n",
    "- Click the `Serial Terminal` tab at the bottom of the window.\n",
    "- Bring in blocks under the camera to detect them.\n",
    "\n",
    "\n",
    "```{image} ./figures/DetectingBlock.png\n",
    ":width: 350\n",
    ":align: center\n",
    "```\n",
    "\n",
    "On Serial Terminal, you will find numbers similar to\n",
    "\n",
    "`2,3,4.041906,-1.517668,-9.712036,181.984062,358.371687,224.128056,4,5.561428,1.540591,-9.330090,160.538206,354.057860,201.279116`\n",
    "\n",
    "The descriptions of the data fields are as follows.\n",
    "- Field 1: Number of AprilTags detected.\n",
    "- Field 2: AprilTag ID\n",
    "- Field 3: x value\n",
    "- Field 4: y value\n",
    "- Field 5: z value\n",
    "- Field 6: Rx value\n",
    "- Field 7: Ry value\n",
    "- Field 8: Rz value\n",
    "- Field 9: AprilTag ID (if there are more than 1 AprilTag)\n",
    "- Field 10: x value\n",
    "- Field 11: y value\n",
    "- Field 12: z value\n",
    "- Field 13: Rx value\n",
    "- Field 14: Ry value\n",
    "- Field 15: Rz value\n",
    "-    :\n",
    "-    :\n",
    "\n",
    "The values are based on large-sized AprilTags. So, the distances returned by the program must be scaled. For example, $z = -9.712$ is incorrect, and it should probably be 11 cm. So, we need to find the scale factor and offset.  \n",
    "\n",
    "We must solve a least squares regression problem to find a linear equation that best fits the measured data. \n",
    "\n",
    "The goal is to find $m$ and $b$ such that $ d = mz + b $ best approximates the linear relationship between $d$ and $z$, where $d$ is the actual distance between the camera lens and the block and $z$ is the OpenMV output.\n",
    "\n",
    ":::{note}\n",
    "Your b should be close to 0.\n",
    ":::\n",
    "\n",
    "Place the camera at the origin and a block with an AprilTag at $(x,y)$ = (10, 0) cm. Place the block on top of another block 1 inch above the ground. \n",
    "\n",
    "\n",
    "```{image} ./figures/OpenMV_Calibration.png\n",
    ":width: 350\n",
    ":align: center\n",
    "```\n",
    "\n",
    "As you move the block 1 cm in the camera's $z$ direction at a time, record the $z$ values returned by the April tag detection in OpenMV IDE.  The $x$ direction on the grid is the $-z$ direction of the camera.\n",
    "\n",
    "**AR: Take measurements and put them in the array below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26906ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Distances from 8 cm to 23 cm\n",
    "# Change the range if needed\n",
    "d = np.arange(0.08, 0.24, 0.01)  \n",
    "# Add your measurements here\n",
    "# Do not add negative signs, e.g.,\n",
    "z = -np.array([4.14, 4.67, 5.16, 5.63, 6.13, 6.64, 7.12, 7.65, 8.13, 8.69, 9.18, 9.73, 10.28, 10.70, 11.30, 11.83])\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023055a",
   "metadata": {},
   "source": [
    "Repeat the same for the $x$ direction (horizontal direction) of the camera - It is the $y$ direction of the grid. You can ignore the offset this time because the pixel at the center of the image is always $(x, y)$ = (0,0).  The offset you observe while taking measurements is due to the misalignment of the camera's $x-y$ plane and the grid's $x-y$ plane.  \n",
    "\n",
    "You don't have to repeat it for the camera's $y$ direction (vertical direction). We can use the same fitting values for the $x$ direction. They should be the same, theoretically.\n",
    "\n",
    "**AR: Take measurements and put them in the array below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025a55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At 0.20 m away\n",
    "d = np.arange(-0.05, 0.06, 0.01) \n",
    "x = np.array([-4.34, -3.52, -2.75, -1.85, -1.13, 0.0, 0.85, 1.68, 2.50, 3.28, 4.12])\n",
    "\n",
    "# Write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234de47-f729-462a-9d38-32db6a1d328a",
   "metadata": {},
   "source": [
    "### Update OpenMV MicroPython\n",
    "\n",
    "Since we are not going to use the orientation of AprilTags, we don't have to send it to the PC.  Update the mz, bz, mx, my values in your MicroPython code.\n",
    "\n",
    "```Python\n",
    "tag_output = list()\n",
    "num_tags = 0\n",
    "\n",
    "# mz is the slope for the z direction\n",
    "# bz is the offset for the z direction\n",
    "# mx is the slope for the x direction\n",
    "# my is the slope for the y direction, and it should be the same as mx\n",
    "mz = 1\n",
    "bz = 0\n",
    "mx = 1\n",
    "my = 1  \n",
    "\n",
    "\n",
    "while(True):\n",
    "    clock.tick()\n",
    "    :\n",
    "    :\n",
    "```\n",
    "\n",
    "Save the code to OpenMV cam as shown below\n",
    "\n",
    "\n",
    "```{image} ./figures/OpenMV_SaveToCam.png\n",
    ":width: 350\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Go to href=\"https://colab.research.google.com/github/stanbaek/ece487/blob/main/docs/Labs/Lab3_CameraCalibration.ipynb\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    "to write your code.\n",
    "\n",
    "## üöö Deliverables\n",
    "\n",
    "###  Deliverable 1 (50 points)\n",
    "\n",
    "Provide the LSE plots for the x-axis and z-axis of the camera.  Provide the mz, bz, mx, and my values.   \n",
    "\n",
    "###  Deliverable 2 (??? points)\n",
    "\n",
    "Have a very nice and relaxing holiday!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e1d63-9425-4082-950a-f3639ddcbc64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
