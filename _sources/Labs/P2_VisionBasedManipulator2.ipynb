{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecaa918",
   "metadata": {},
   "source": [
    "# ðŸš© P2: Robot with Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26b7f4",
   "metadata": {},
   "source": [
    "Our goal is to find the line that best approximates the data.\n",
    "\n",
    "Place the camera at the orign and a block with an AprilTag at $(x,y)$ = (10, 0) cm. Place the block on top of another block for 1 inch above the ground. \n",
    "\n",
    "As you move the block 1 cm in the $x$ direction at a time, record the $z$ values returned by the apriltag detection in OpenMV IDE.  Although it is the $x$ direction on the grid, it is actually the $z$ direction of the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(0.1, 0.28, 0.01)  # 10 cm - 28 cm incremented by 1 cm\n",
    "# add your measurements here\n",
    "x = np.array([])\n",
    "A = np.vstack([x, np.ones(len(x))]).T\n",
    "m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "print(m, c)\n",
    "plt.plot(x, y, 'o', label='Measured data', markersize=10)\n",
    "plt.plot(x, m*x + c, 'r', label='Fitted line')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fc258",
   "metadata": {},
   "source": [
    "Repeat the same for the $x$ direction (horizontal direction) of the camera - It is the $y$ direction of the grid. You can ignore the offset this time, because the pixel at the center of the image is always $(x, y)$ = (0,0).  The offset you observe while taking measurements are due to the misalignment of the camera's $x-y$ plane and the grid's $x-y$ plane.  \n",
    "\n",
    "You don't have to repeat it for camera's $y$ direction (vertical direction). We can use the same fitting values for the $x$ direction. They should be them same theoretically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ea863",
   "metadata": {},
   "source": [
    "### Update OpenMV MicroPython\n",
    "\n",
    "Since we are not going to use the orientation of AprilTags, we don't have to send it to the PC.  Update your MicroPython code as follows\n",
    "\n",
    "```\n",
    "tag_output = list()\n",
    "num_tags = 0\n",
    "\n",
    "# add the following lines and provide the values\n",
    "# mz is the slope for the z direction\n",
    "# cz is the offset for the z direction\n",
    "# mx is the slope for the x direction\n",
    "# my is the slope for the y direction, and it should be the same as mx\n",
    "mz =\n",
    "cz =\n",
    "mx =\n",
    "my =  \n",
    "\n",
    "\n",
    "while(True):\n",
    "    clock.tick()\n",
    "    :\n",
    "    :\n",
    "    for tag in tags:\n",
    "        img.draw_rectangle(tag.rect(), color = (255, 0, 0))\n",
    "        img.draw_cross(tag.cx(), tag.cy(), color = (0, 255, 0))\n",
    "        # update the below line\n",
    "        print_args =  (tag.id(), tag.x_translation()*mx, tag.y_translation()*my, tag.z_translation()*mz + cz)\n",
    "        # Translation units are unknown. Rotation units are in degrees.\n",
    "        # upldate the below line\n",
    "        print(\",%d,%f,%f,%f\" % print_args, end='')\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302d73f",
   "metadata": {},
   "source": [
    "Save the code to OpenMV cam as shown below\n",
    "\n",
    "<img src=\"figs/OpenMV_SaveToCam.png\" alt=\"Blocks\" style=\"width: 350px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcc879",
   "metadata": {},
   "source": [
    "### Search for Blocks\n",
    "\n",
    "We need to search for blocks with AprilTags using the OpenMV cammera attached to the robotic arm. Add the following code to the constructor (`def --init__(...)`) into your `xArm.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.B0 = 0.090\n",
    "        self.L1 = 0.010\n",
    "        self.L2 = 0.105\n",
    "        self.L3 = 0.088\n",
    "        self.L4 = 0.170\n",
    "        #self.camera_distance = 0.133 # distance of OpenMV cam from Joint5\n",
    "        self.camera_dist_offset = 0.037  # L4 - camera_distance\n",
    "        \n",
    "                \n",
    "        \n",
    "        # ADD this for OpenMV Cam\n",
    "        self.mvcam = None\n",
    "        self.curr_joint_angles = None\n",
    "        \n",
    "        # dictionary for block locations\n",
    "        # for short tutorial for Python dictionary, visit https://www.w3schools.com/python/python_dictionaries.asp\n",
    "        self.block_locations = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5dabe",
   "metadata": {},
   "source": [
    "Add the following `Tag` class before `class XArm(rbt.DHRobot):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(NamedTuple):\n",
    "    \"\"\" Data structure for AprilTag\n",
    "    \"\"\"\n",
    "    id: int\n",
    "    x: float\n",
    "    y: float\n",
    "    z: float\n",
    "        \n",
    "        \n",
    "class XArm(rbt.DHRobot):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e568d23",
   "metadata": {},
   "source": [
    "Add the following two functions into your xArm.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def connect_mvcam(self):\n",
    "\n",
    "        if isinstance(self.mvcam, serial.Serial):\n",
    "            print(f\"The robot is already connected to {self.comm.port}\")\n",
    "            return\n",
    "\n",
    "        ports = list_ports.comports()\n",
    "        comport = None\n",
    "\n",
    "        for port, desc, hwid in sorted(ports):\n",
    "            if 'OpenMV Cam' in desc:\n",
    "                comport = port\n",
    "                print(\"Info: OpenMV Cam is connected to\")\n",
    "                print(\"{}: {} [{}]\".format(port, desc, hwid))\n",
    "                break\n",
    "\n",
    "        if comport is None:\n",
    "            print(\"Error: Serial port for OpenMV is not available.\")\n",
    "            print(\"OpenMV Cam is not connected.\")\n",
    "            return\n",
    "\n",
    "        # A serial port is found.  Try to connect.\n",
    "        try:\n",
    "            self.mvcam = serial.Serial(\n",
    "                port=comport,\n",
    "                baudrate=115200,\n",
    "                parity=serial.PARITY_NONE,\n",
    "                stopbits=serial.STOPBITS_ONE,\n",
    "                bytesize=serial.EIGHTBITS,\n",
    "                timeout=0.01\n",
    "            )\n",
    "            print(f\"Info: OpenMV is connected to {comport}\")\n",
    "\n",
    "        except serial.SerialException:\n",
    "            print(\"Error: Serial port is not available.\")\n",
    "            print(\"OpenMV is not connected.\")\n",
    "            \n",
    "            \n",
    "    def search_for_blocks(self, duration_ms, pose, steps):\n",
    "        \"\"\" Search for blocks with AprilTags.\n",
    "        The robotic arm will be moved from the current pose to pose (argument) for duration_ms.\n",
    "        During the transition from the current pose to the desired pose, the robot will stop moving to search\n",
    "        for blocks. The number of stops during the transition is given by the steps argument.\n",
    "        Once it finds a block, it will calculate the pose of the block wrt Frame 0 using the\n",
    "        current joint angles and the position of the block wrt to the camera frame. Then, it will update\n",
    "        self.block_locations with the pose of the block wrt Frame 0.\n",
    "        If a block is detected multiple times, self.block_locations will be updated with the latest location.\n",
    "        :param duration_ms: time in milliseconds to travel from the current\n",
    "        position to the desired position.\n",
    "        :param pose: 1x5 array of the pose of the tooltip.\n",
    "        The first three elements are the position of the tooltip in meters,\n",
    "        and the last two elements are the wrist angles in radians.\n",
    "        :param steps: the number of stops during the transition\n",
    "        :return: None\n",
    "\n",
    "        Example:\n",
    "        robot = XArm(simulation_only=False)\n",
    "        robot.connect_mvcam()\n",
    "        robot.connect()\n",
    "\n",
    "        duration_ms = 2000\n",
    "        steps = 20\n",
    "\n",
    "        robot.move_to_initial_pose(duration_ms)\n",
    "        pose = (0.12, -0.15, 0.12, -pi/4, 0)\n",
    "        robot.search_for_blocks(duration_ms, pose, steps)\n",
    "        \"\"\"\n",
    "        \n",
    "        joint_angles = self.invkine(pose)\n",
    "\n",
    "        if joint_angles is None:\n",
    "            print(\"No solution to inverse kinematics for \", pose)\n",
    "            return\n",
    "\n",
    "        angle_step = (joint_angles[0, :] - self.curr_joint_angles)/steps\n",
    "        dtime_ms = duration_ms//steps\n",
    "\n",
    "        for i in range(steps):\n",
    "            self.curr_joint_angles += angle_step\n",
    "            self.move_joints(dtime_ms, self.curr_joint_angles, wait=True)\n",
    "\n",
    "            # print(np.rad2deg(self.curr_joint_angles))\n",
    "            data = self.mvcam.readline().decode('ascii').strip().split(',')\n",
    "\n",
    "            if len(data) < 2:\n",
    "                continue\n",
    "\n",
    "            num_tags = int(data.pop(0))\n",
    "            tags = list()\n",
    "            for i in range(num_tags):\n",
    "                tagid = int(data[4*i])\n",
    "                x, y, z = map(float, data[4*i+1:4+4*i])\n",
    "\n",
    "                \"\"\" Write your code to find the location of the block wrt the \n",
    "                inertial reference frame, Frame {0}\n",
    "                \"\"\"\n",
    "                # Find the transformation matrix of Tool Frame wrt Frame 0\n",
    "                # Hint: You need only one line of code here\n",
    "                \n",
    "                \n",
    "                # Find the transformation matrix of the block wrt Frame 0\n",
    "                # Hint: you need only one line of code (Look at Lab1)\n",
    "                \n",
    "                \n",
    "                # Add the location (translation, or the x-,y-,z-coordinates) of the block.\n",
    "                self.block_locations[tagid] = 0  # replace 0 with the location of the block.  \n",
    "                \n",
    "            # comment out the following line if you don't want to print    \n",
    "            [print(key, ', ', self.block_locations[key]) for key in self.block_locations.keys()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f1764",
   "metadata": {},
   "source": [
    "Complete the `search_for_blocks` function. Use the following code to test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7936ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search_for_blocks():\n",
    "\n",
    "    # Set simulation_only=True to test the OpenMV cam.\n",
    "    # Set it False to test the camera while the robot is moving.    \n",
    "    robot = XArm(simulation_only=True)\n",
    "    robot.connect_mvcam()\n",
    "    robot.connect()\n",
    "\n",
    "    duration_ms = 2000\n",
    "    steps = 0  # pick a number for steps.\n",
    "\n",
    "    robot.move_to_initial_pose(duration_ms)\n",
    "    \n",
    "    pose = (0, 0, 0, 0, 0)  # pick the start pose for searching \n",
    "    robot.moveto(duration_ms, pose, wait=True)\n",
    "\n",
    "    pose = (0, 0, 0, 0, 0)  # pick the destination pose for searching\n",
    "    robot.search_for_blocks(duration_ms, pose, steps)\n",
    "    \n",
    "    pose = (0, 0, 0, 0, 0) # pick the next destination pose for searching\n",
    "    robot.search_for_blocks(duration_ms, pose, steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b911a",
   "metadata": {},
   "source": [
    "### Deliverable 1 (10 points)\n",
    "\n",
    "Your code should be able to find and identify blocks placed at,\n",
    "- (22, 0, 0)\n",
    "- (30, 0, 0)\n",
    "- (19, -10, 0)\n",
    "- (26, -11, 0)\n",
    "- (25, 18, 0)\n",
    "\n",
    "**Report the console output printed by the following code inside `search_for_blocks`.**\n",
    "\n",
    "`[print(key, ', ', self.block_locations[key]) for key in self.block_locations.keys()]`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98883c8c",
   "metadata": {},
   "source": [
    "### Deliverable 2 (40 points)\n",
    "As you can find, the localization accuracy is sometimes very poor depending highly on where the block is located in the image frame. If the block is at the center of the image frame, it is usually pretty accurate. If the block is located at the corner or side of the image frame, it has a large error. How would you improve the accuracy? \n",
    "\n",
    "A few ideas that you may want to consider (not necessarily good ideas)\n",
    "- Average out all the measurements\n",
    "- Based on the locations you initially have, move the arm where a block can be located at the center of the image frame, and retake a measurement.\n",
    "\n",
    "**Report the following in your final report**\n",
    "- Implementation of your idea in xArm.py.  You can modify the `search_for_blocks` method or create your own method.\n",
    "- Justification of the idea.\n",
    "- Measurements.\n",
    "- Analysis of your data.\n",
    "\n",
    "In the real engineering world, there are always multiple solutions available. There is no right or wrong solution, but there are better solutions. I would like to see your ideas, implementation, justification, and analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada2cd8",
   "metadata": {},
   "source": [
    "### Deliverable 3 (30+ points)\n",
    "\n",
    "**Demo (30 pts):** Search for a block with AprilTag ID 0 (let's call it Block 0) and Block 1, and place Block 0 on top of Block 1.\n",
    "\n",
    "The blocks are placed anywhere near the five locations ($\\pm 5$ cm) in Deliverable 1. \n",
    "\n",
    "**Demo (3 - 7 bonus pts):** Move multiple blocks (Blocks 0 - 3) on top of Block 4.\n",
    "- 2 blocks: + 3 points\n",
    "- 3 blocks: + 5 points\n",
    "- 3 blocks: + 7 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23facea8",
   "metadata": {},
   "source": [
    "### Deliverable 4 (20 points)\n",
    "\n",
    "**Submit final report.**\n",
    "\n",
    "Your report should include the following sections or similar:\n",
    "\n",
    "- Objective\n",
    "- Methods\n",
    "- Results and Analyses\n",
    "- Conclusion \n",
    "\n",
    "There are no requirements for font size, page margins, number of pages, etc. Write a professional, senior-level report. \n",
    "Do not use _you_ in your report.  \n",
    "Hint: I love figures and tables as long as they are legible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c76da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
